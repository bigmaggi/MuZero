{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "2.0.1\n",
      "0.29.0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "import unittest\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "import time  # Add this import at the beginning of the file\n",
    "from tqdm.notebook import trange\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# Set up device\n",
    "# device = torch.device(\"mps\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\")\n",
    "print(device)\n",
    "print(torch.__version__)\n",
    "print(gym.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepresentationFunction(nn.Module):\n",
    "    def __init__(self, input_size, representation_size):\n",
    "        super(RepresentationFunction, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, representation_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class InitialRepresentationFunction(nn.Module):\n",
    "    def __init__(self, input_size, representation_size):\n",
    "        super(InitialRepresentationFunction, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, representation_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x.to(self.fc1.weight.device)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicsFunction(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_dim):\n",
    "        super(DynamicsFunction, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Define layers for the dynamics function\n",
    "        self.fc_state = nn.Linear(state_size, hidden_dim)\n",
    "        self.fc_action = nn.Linear(action_size, hidden_dim)\n",
    "        self.fc_hidden = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc_output = nn.Linear(hidden_dim, state_size)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state = torch.flatten(state, start_dim=1)\n",
    "        action = torch.flatten(action, start_dim=1)\n",
    "    \n",
    "        # Concatenate state and action along the last dimension\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "\n",
    "\n",
    "        x_state = self.fc_state(state)\n",
    "        x_action = self.fc_action(action)\n",
    "\n",
    "        x = torch.relu(self.fc_hidden(torch.cat([x_state, x_action], dim=1)))\n",
    "        x = self.fc_output(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionFunction(nn.Module):\n",
    "    def __init__(self, representation_size, action_space_size):\n",
    "        super(PredictionFunction, self).__init__()\n",
    "        self.fc = nn.Linear(representation_size, 128)  # Note the input size here\n",
    "        self.out = nn.Linear(128, action_space_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc(x))\n",
    "        x = self.out(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space_size,\n",
    "        input_size,\n",
    "        action_space_size,\n",
    "        representation_size=128,\n",
    "        batch_size=64,\n",
    "        learning_rate_prediction=0.001,\n",
    "        update_interval=100,\n",
    "        checkpoint_interval=1000,\n",
    "        replay_buffer_capacity=10000,\n",
    "    ):\n",
    "        self.input_size = input_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.representation_size = representation_size\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate_prediction = learning_rate_prediction\n",
    "        self.update_interval = update_interval\n",
    "        self.checkpoint_interval = checkpoint_interval\n",
    "        self.replay_buffer_capacity = replay_buffer_capacity\n",
    "        self.observation_space_size = observation_space_size\n",
    "        self.environment_name = \"Breakout-v4\"\n",
    "        self.render_mode = 'human'\n",
    "        self.max_total_steps = 100000\n",
    "        self.epsilon = 0.1\n",
    "        self.state_size = 4\n",
    "        self.action_size = 2\n",
    "        self.hidden_dim = 128\n",
    "        self.gamma = 0.99\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, hidden_state, reward, terminal, action_space):\n",
    "        self.hidden_state = hidden_state\n",
    "        self.reward = reward\n",
    "        self.terminal = terminal\n",
    "        self.children = [None] * action_space\n",
    "        self.total_value = [0] * action_space\n",
    "        self.visit_count = [0] * action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    def __init__(self, action_space_size, representation_function, dynamics_function, prediction_function):\n",
    "        self.action_space_size = action_space_size\n",
    "        self.num_simulations = config.num_simulations\n",
    "        self.discount = config.mcts_discount\n",
    "        self.root = None\n",
    "        self.dynamics_function = dynamics_function\n",
    "        self.prediction_function = prediction_function\n",
    "        self.exploration_constant = config.exploration_constant\n",
    "\n",
    "    def UCB_score(self, node, action):\n",
    "        if node.visit_count[action] == 0:\n",
    "            return float('inf')\n",
    "        else:\n",
    "            # Use the model to predict the value of the action\n",
    "            state = node.hidden_state\n",
    "            predicted_values = self.prediction_function(state)\n",
    "            Q = predicted_values[0][action]\n",
    "            U = self.exploration_constant * math.sqrt(math.log(sum(node.visit_count)) / node.visit_count[action])\n",
    "            return Q + U\n",
    "\n",
    "    def expand(self, node, action):\n",
    "        next_state, reward = self.dynamics_function(node.hidden_state, torch.tensor([action], dtype=torch.float32).to(device))\n",
    "        next_state = next_state.clone().detach().to(device)\n",
    "        reward = reward.item()\n",
    "        return Node(next_state, reward, False, self.action_space_size)\n",
    "\n",
    "    def backpropagate(self, leaf_value, path):\n",
    "        for node, action in reversed(path):\n",
    "            node.visit_count[action] += 1\n",
    "            node.total_value[action] += leaf_value\n",
    "            leaf_value *= self.discount\n",
    "\n",
    "    def run(self, initial_state):\n",
    "        # Create root node with initial state\n",
    "        initial_hidden_state = initial_state\n",
    "        self.root = Node(initial_hidden_state, 0, False, self.action_space_size)\n",
    "\n",
    "        for _ in range(self.num_simulations):\n",
    "            node, path = self.root, []\n",
    "            while node is not None:\n",
    "                best_action = max(range(self.action_space_size), key=lambda a: self.UCB_score(node, a))\n",
    "                path.append((node, best_action))\n",
    "                if node.children[best_action] is None:\n",
    "                    node.children[best_action] = self.expand(node, best_action)\n",
    "                node = node.children[best_action]\n",
    "\n",
    "            leaf = path[-1][0]\n",
    "            self.backpropagate(leaf.reward, path)\n",
    "\n",
    "        return self.root\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n",
    "        return state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "        self.action_space = self.env.action_space.n\n",
    "        self.action_space_size = self.env.action_space.n\n",
    "        self.dynamics_input_size = self.config.representation_size + self.config.action_space_size\n",
    "        self.representation_function = RepresentationFunction(\n",
    "            input_size=self.config.input_size,\n",
    "            representation_size=self.config.representation_size,\n",
    "        ).to(self.device)\n",
    "        self.dynamics_function = DynamicsFunction(\n",
    "            state_size=self.config.representation_size,\n",
    "            action_size=self.config.action_space_size,\n",
    "            hidden_dim=64\n",
    "        ).to(self.device)\n",
    "        self.prediction_function = PredictionFunction(\n",
    "            representation_size=self.config.representation_size,\n",
    "            action_space_size=self.config.action_space_size\n",
    "        ).to(self.device)\n",
    "        self.replay_buffer = ReplayBuffer(config.replay_buffer_capacity)\n",
    "        self.total_steps = 0\n",
    "        self.training_steps_completed = 0\n",
    "        self.optimizer_representation = optim.Adam(self.representation_function.parameters())\n",
    "        self.optimizer_prediction = optim.Adam(self.prediction_function.parameters())\n",
    "        self.epsilon_start = config.epsilon\n",
    "        self.epsilon_final = 0.01\n",
    "        self.epsilon_decay_duration = config.max_total_steps // 2\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    \n",
    "    def get_epsilon(self, step):\n",
    "        # Linearly decay epsilon\n",
    "        epsilon_start = 1.0\n",
    "        epsilon_final = 0.1\n",
    "        epsilon_decay_duration = self.config.max_total_steps // 2\n",
    "        epsilon = epsilon_final + (epsilon_start - epsilon_final) * max(0, (epsilon_decay_duration - step) / epsilon_decay_duration)\n",
    "        return epsilon\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(self.representation_function.state_dict(), 'representation_function_best.pth')\n",
    "        torch.save(self.dynamics_function.state_dict(), 'dynamics_function_best.pth')\n",
    "        torch.save(self.prediction_function.state_dict(), 'prediction_function_best.pth')\n",
    "\n",
    "    def concat_state_action(self, state_repr, action_tensor):\n",
    "        action_one_hot = F.one_hot(action_tensor, num_classes=self.action_space).float()\n",
    "        state_action_repr = torch.cat([state_repr, action_one_hot], dim=-1)\n",
    "        return state_action_repr\n",
    "    \n",
    "    def get_action(self, state_repr, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.choice(self.action_space_size)\n",
    "    \n",
    "        # Exploitation\n",
    "        q_values = torch.zeros(self.action_space_size, device=self.device)\n",
    "        for action in range(self.action_space_size):\n",
    "            action_one_hot = F.one_hot(torch.tensor([action]), num_classes=self.action_space_size).to(self.device)\n",
    "            # Concatenate state and action and feed to prediction function\n",
    "            state_action = torch.cat([state_repr, action_one_hot], dim=1)\n",
    "            state_action = state_action.unsqueeze(0)  # Add an extra dimension for the batch size\n",
    "            q_value = self.prediction_function(state_action)\n",
    "            q_values[action] = q_value.item()\n",
    "    \n",
    "        action = torch.argmax(q_values).item()  # Choose the action with the highest Q value\n",
    "        assert action < self.action_space_size, f\"Invalid action {action} selected\"\n",
    "        return action\n",
    "    \n",
    "    def select_action(self, state_repr, epsilon):\n",
    "        if np.random.rand() < epsilon:  # Exploration\n",
    "            return random.choice(range(self.action_space_size))\n",
    "\n",
    "        # Exploitation\n",
    "        q_values = self.prediction_function(state_repr)  # Using state_repr here\n",
    "        action = torch.argmax(q_values).item()\n",
    "        return action\n",
    "\n",
    "    \n",
    "\n",
    "    def train(self):\n",
    "        # Sample a batch from the replay buffer\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.replay_buffer.sample(self.config.batch_size)\n",
    "    \n",
    "        # Convert batches to tensors\n",
    "        state_batch = torch.tensor(state_batch, dtype=torch.float32).to(self.device)\n",
    "        action_batch_tensor = torch.tensor(action_batch, dtype=torch.int64).to(self.device).unsqueeze(1)\n",
    "        reward_batch = torch.tensor(reward_batch, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "        next_state_batch = torch.tensor(next_state_batch, dtype=torch.float32).to(self.device)\n",
    "        done_batch = torch.tensor(done_batch, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "    \n",
    "        # Compute state representations\n",
    "        state_repr = self.representation_function(state_batch)\n",
    "        next_state_repr = self.representation_function(next_state_batch)\n",
    "    \n",
    "        # One-hot encode the actions\n",
    "        action_batch_one_hot = F.one_hot(action_batch_tensor.squeeze(), num_classes=self.config.action_space_size).float().to(self.device)\n",
    "    \n",
    "        # Forward pass through the dynamics function\n",
    "        predicted_next_state_repr = self.dynamics_function(state_repr, action_batch_one_hot)\n",
    "    \n",
    "        # Compute the dynamics loss\n",
    "        dynamics_loss = F.mse_loss(predicted_next_state_repr, next_state_repr)\n",
    "    \n",
    "        # Compute the prediction loss\n",
    "        q_values = self.prediction_function(state_repr).gather(1, action_batch_tensor)\n",
    "        next_q_values = self.prediction_function(next_state_repr).detach()\n",
    "        max_next_q_values = next_q_values.max(1)[0].unsqueeze(1)\n",
    "        target_q_values = reward_batch + self.config.gamma * max_next_q_values * (1 - done_batch)\n",
    "        prediction_loss = F.mse_loss(q_values, target_q_values)\n",
    "    \n",
    "        # Zero the gradients for both the dynamics and prediction networks\n",
    "        self.optimizer_representation.zero_grad()\n",
    "        self.optimizer_prediction.zero_grad()\n",
    "    \n",
    "        # Backpropagate the total loss\n",
    "        total_loss = dynamics_loss + prediction_loss\n",
    "        total_loss.backward()\n",
    "\n",
    "        if total_loss < self.best_loss:\n",
    "            self.best_loss = total_loss\n",
    "            self.save_checkpoint()\n",
    "    \n",
    "        # Update the parameters for both the dynamics and prediction networks\n",
    "        self.optimizer_representation.step()\n",
    "        self.optimizer_prediction.step()\n",
    "    \n",
    "        return total_loss.item()  # Return the combined loss\n",
    "\n",
    "    def run(self):\n",
    "        self.env = gym.make(self.config.environment_name, render_mode=self.config.render_mode)\n",
    "        state, _ = self.env.reset()\n",
    "        state_repr = self.representation_function(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device))\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "    \n",
    "        # Progress bar initialization\n",
    "        progress_bar = tqdm(total=self.config.max_total_steps, desc=\"Training progress\")\n",
    "    \n",
    "        while self.total_steps < self.config.max_total_steps:\n",
    "            # Get epsilon for this step\n",
    "            epsilon = self.get_epsilon(self.total_steps)\n",
    "    \n",
    "            # Select action using the epsilon-greedy policy\n",
    "            action = self.select_action(state_repr, epsilon)\n",
    "            \n",
    "            next_state, reward, done, _, _ = self.env.step(action)\n",
    "            next_state_repr = self.representation_function(torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(self.device))\n",
    "    \n",
    "            # Append the transition to the replay buffer\n",
    "            self.replay_buffer.add(state, action, reward, next_state, done)  # Use state and next_state, not state_repr and next_state_repr\n",
    "    \n",
    "            state = next_state\n",
    "            state_repr = next_state_repr\n",
    "            episode_reward += reward\n",
    "            self.total_steps += 1\n",
    "    \n",
    "            if done:\n",
    "                state, _ = self.env.reset()\n",
    "                state_repr = self.representation_function(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device))\n",
    "    \n",
    "            if len(self.replay_buffer) >= self.config.batch_size and self.total_steps % self.config.update_interval == 0:\n",
    "                print(\"Updating parameters...\")\n",
    "                loss = self.train()\n",
    "                self.training_steps_completed += 1\n",
    "                if self.training_steps_completed % self.config.checkpoint_interval == 0:\n",
    "                    self.save_checkpoint()\n",
    "    \n",
    "                if loss is not None:\n",
    "                    print(f\"Step: {self.total_steps}, Loss: {loss:.4f}\")\n",
    "    \n",
    "            # Update the progress bar\n",
    "            progress_bar.update(1)\n",
    "    \n",
    "        # Close the progress bar\n",
    "        progress_bar.close()\n",
    "    \n",
    "        self.env.close()\n",
    "\n",
    "\n",
    "\n",
    "    def populate_initial_buffer(self):\n",
    "        state, _ = self.env.reset()\n",
    "        state_repr = self.representation_function(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device))\n",
    "\n",
    "        for _ in range(self.config.initial_buffer_size):\n",
    "            action = self.env.action_space.sample()\n",
    "            next_state, reward, done, _, _ = self.env.step(action)\n",
    "            next_state_repr = self.representation_function(torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(self.device))\n",
    "            # Append the transition to the replay buffer\n",
    "            self.replay_buffer.add(state_repr.detach().cpu().numpy(), action, reward, next_state_repr.detach().cpu().numpy(), done)\n",
    "\n",
    "            if done:\n",
    "                state, _ = self.env.reset()\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "    def update_parameters(self):\n",
    "        # Sample a batch of transitions from the replay buffer\n",
    "        batch = random.sample(self.replay_buffer, self.config.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    \n",
    "        # Convert the variables to tensors and move them to the appropriate device\n",
    "        states = torch.stack(states).to(self.device)\n",
    "        actions = torch.tensor(actions).unsqueeze(-1).to(self.device)  # Reshape actions to match states\n",
    "        rewards = torch.tensor(rewards).to(self.device)\n",
    "        next_states = torch.stack(next_states).to(self.device)\n",
    "        dones = torch.tensor(dones).to(self.device)\n",
    "    \n",
    "        # Compute the loss for each of the three networks\n",
    "        # 1. Representation loss\n",
    "        predicted_state_repr = self.dynamics_function(states, actions)\n",
    "        target_state_repr = self.representation_function(next_states)\n",
    "        repr_loss = F.mse_loss(predicted_state_repr, target_state_repr.detach())\n",
    "    \n",
    "        # 2. Dynamics loss\n",
    "        predicted_next_state_repr, predicted_reward = self.dynamics_function(states, actions)\n",
    "        dynamics_loss = F.mse_loss(predicted_next_state_repr, target_state_repr.detach()) + F.mse_loss(predicted_reward, rewards.unsqueeze(-1).detach())\n",
    "    \n",
    "        # 3. Prediction loss\n",
    "        predicted_action_values, predicted_reward = self.prediction_function(states)\n",
    "        target_action_values = self.target_prediction_function(target_state_repr).detach()\n",
    "        pred_loss = F.mse_loss(predicted_action_values, target_action_values) + F.mse_loss(predicted_reward, rewards.unsqueeze(-1).detach())\n",
    "    \n",
    "        # Combine the three losses and backpropagate\n",
    "        loss = repr_loss + dynamics_loss + pred_loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "        # Update the target networks\n",
    "        self._update_target_network(self.prediction_function, self.target_prediction_function, self.config.soft_update_tau)\n",
    "    \n",
    "        return loss.item()\n",
    " \n",
    "    def play(self, num_episodes=1):\n",
    "        # Load the saved models\n",
    "        self.load_checkpoint()\n",
    "\n",
    "        # Play the specified number of episodes\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            state_repr = self.representation_function(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device))\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                action = self.select_action(state_repr, epsilon=0.0)\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                next_state_repr = self.representation_function(torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(self.device))\n",
    "\n",
    "                state = next_state\n",
    "                state_repr = next_state_repr\n",
    "                episode_reward += reward\n",
    "\n",
    "                self.env.render()  # Uncomment this line if you want to visualize the gameplay\n",
    "\n",
    "            print(f\"Episode {episode + 1}: Total Reward = {episode_reward}\")\n",
    "\n",
    "        self.env.close()\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(self.representation_function.state_dict(), 'representation_function.pth')\n",
    "        torch.save(self.dynamics_function.state_dict(), 'dynamics_function.pth')\n",
    "        torch.save(self.prediction_function.state_dict(), 'prediction_function.pth')\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        if os.path.isfile('representation_function.pth'):\n",
    "            self.representation_function.load_state_dict(torch.load('representation_function.pth'))\n",
    "        if os.path.isfile('dynamics_function.pth'):\n",
    "            self.dynamics_function.load_state_dict(torch.load('dynamics_function.pth'))\n",
    "        if os.path.isfile('prediction_function.pth'):\n",
    "            self.prediction_function.load_state_dict(torch.load('prediction_function.pth'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EEEE\n",
      "======================================================================\n",
      "ERROR: test_dynamics_function (__main__.TestFunctions)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/l5/r3g8kwp54k9f9vc409rvbkrh0000gn/T/ipykernel_9196/1157417371.py\", line 7, in setUp\n",
      "    self.env = gym.make(self.config.environment_name, render_mode=self.config.render_mode)\n",
      "  File \"/Users/bigmaggi/miniforge3/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 741, in make\n",
      "    env_spec = _find_spec(id)\n",
      "  File \"/Users/bigmaggi/miniforge3/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 527, in _find_spec\n",
      "    _check_version_exists(ns, name, version)\n",
      "  File \"/Users/bigmaggi/miniforge3/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 393, in _check_version_exists\n",
      "    _check_name_exists(ns, name)\n",
      "  File \"/Users/bigmaggi/miniforge3/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 370, in _check_name_exists\n",
      "    raise error.NameNotFound(\n",
      "gymnasium.error.NameNotFound: Environment `Breakout` doesn't exist.\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_initial_representation_function (__main__.TestFunctions)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/l5/r3g8kwp54k9f9vc409rvbkrh0000gn/T/ipykernel_9196/1157417371.py\", line 7, in setUp\n",
      "    self.env = gym.make(self.config.environment_name, render_mode=self.config.render_mode)\n",
      "  File \"/Users/bigmaggi/miniforge3/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 741, in make\n",
      "    env_spec = _find_spec(id)\n",
      "  File \"/Users/bigmaggi/miniforge3/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 527, in _find_spec\n",
      "    _check_version_exists(ns, name, version)\n",
      "  File \"/Users/bigmaggi/miniforge3/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 393, in _check_version_exists\n",
      "    _check_name_exists(ns, name)\n",
      "  File \"/Users/bigmaggi/miniforge3/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 370, in _check_name_exists\n",
      "    raise error.NameNotFound(\n",
      "gymnasium.error.NameNotFound: Environment `Breakout` doesn't exist.\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_prediction_function (__main__.TestFunctions)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/l5/r3g8kwp54k9f9vc409rvbkrh0000gn/T/ipykernel_9196/1157417371.py\", line 7, in setUp\n",
      "    self.env = gym.make(self.config.environment_name, render_mode=self.config.render_mode)\n",
      "  File \"/Users/bigmaggi/miniforge3/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 741, in make\n",
      "    env_spec = _find_spec(id)\n",
      "  File \"/Users/bigmaggi/miniforge3/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 527, in _find_spec\n",
      "    _check_version_exists(ns, name, version)\n",
      "  File \"/Users/bigmaggi/miniforge3/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 393, in _check_version_exists\n",
      "    _check_name_exists(ns, name)\n",
      "  File \"/Users/bigmaggi/miniforge3/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 370, in _check_name_exists\n",
      "    raise error.NameNotFound(\n",
      "gymnasium.error.NameNotFound: Environment `Breakout` doesn't exist.\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_representation_function (__main__.TestFunctions)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/l5/r3g8kwp54k9f9vc409rvbkrh0000gn/T/ipykernel_9196/1157417371.py\", line 7, in setUp\n",
      "    self.env = gym.make(self.config.environment_name, render_mode=self.config.render_mode)\n",
      "  File \"/Users/bigmaggi/miniforge3/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 741, in make\n",
      "    env_spec = _find_spec(id)\n",
      "  File \"/Users/bigmaggi/miniforge3/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 527, in _find_spec\n",
      "    _check_version_exists(ns, name, version)\n",
      "  File \"/Users/bigmaggi/miniforge3/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 393, in _check_version_exists\n",
      "    _check_name_exists(ns, name)\n",
      "  File \"/Users/bigmaggi/miniforge3/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 370, in _check_name_exists\n",
      "    raise error.NameNotFound(\n",
      "gymnasium.error.NameNotFound: Environment `Breakout` doesn't exist.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.003s\n",
      "\n",
      "FAILED (errors=4)\n"
     ]
    }
   ],
   "source": [
    "class TestFunctions(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        observation_space_size = 4\n",
    "        input_size = 4\n",
    "        action_space_size = 2\n",
    "        self.config = Config(observation_space_size, input_size, action_space_size)\n",
    "        self.env = gym.make(self.config.environment_name, render_mode=self.config.render_mode)\n",
    "        self.representation_size = 128\n",
    "        action_space_size = self.env.action_space.n\n",
    "        self.agent = Agent(self.config, self.representation_size, self.action_space_size)\n",
    "        self.representation_function = RepresentationFunction(self.env.observation_space.shape[0], self.representation_size).to(device)\n",
    "        self.dynamics_input_size = representation_size + self.env.action_space.n\n",
    "        self.dynamics_function = DynamicsFunction(self.dynamics_input_size, representation_size, self.env.action_space.n).to(device)\n",
    "        self.prediction_function = PredictionFunction(self.representation_size, action_space_size).to(device)\n",
    "\n",
    "    def test_representation_function(self):\n",
    "        representation_function = RepresentationFunction(self.env.observation_space.shape[0], self.representation_size).to(device)\n",
    "        input_tensor = torch.randn(1, self.env.observation_space.shape[0]).to(device)\n",
    "        output_tensor = representation_function(input_tensor)\n",
    "        self.assertEqual(output_tensor.shape, (1, self.representation_size))\n",
    "\n",
    "    def test_initial_representation_function(self):\n",
    "        state, _ = self.env.reset()\n",
    "        state_tensor = torch.tensor(state).float().unsqueeze(0).to(device)\n",
    "        output_tensor = self.agent.representation_function(state_tensor)\n",
    "        self.assertEqual(output_tensor.shape, (1, self.representation_size))\n",
    "\n",
    "    def test_dynamics_function(self):\n",
    "        state_repr = torch.randn(1, self.representation_size).to(device)\n",
    "        action = torch.randn(1, self.env.action_space.n).to(device)\n",
    "        output_tensor = self.agent.dynamics_function(state_repr, action)\n",
    "        self.assertEqual(output_tensor.shape, (1, self.representation_size))\n",
    "\n",
    "    def test_prediction_function(self):\n",
    "        input_tensor = torch.randn(1, self.representation_size).to(device)\n",
    "        output_tensor = self.agent.prediction_function(input_tensor)\n",
    "        self.assertEqual(output_tensor.shape, (1, self.env.action_space.n))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and run the test case\n",
    "    suite = unittest.TestLoader().loadTestsFromTestCase(TestFunctions)\n",
    "    result = unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run it all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameNotFound",
     "evalue": "Environment `Breakout` doesn't exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameNotFound\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m       \u001b[39m# Create the environment\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mmake(\u001b[39m\"\u001b[39;49m\u001b[39mBreakout-v4\u001b[39;49m\u001b[39m\"\u001b[39;49m, render_mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhuman\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m     \u001b[39m# Define configuration parameters\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     config \u001b[39m=\u001b[39m Config(\n\u001b[1;32m      7\u001b[0m         observation_space_size\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],\n\u001b[1;32m      8\u001b[0m         action_space_size\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m         input_size\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],\n\u001b[1;32m     16\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/gymnasium/envs/registration.py:741\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mid\u001b[39m, \u001b[39mstr\u001b[39m)\n\u001b[1;32m    740\u001b[0m     \u001b[39m# The environment name can include an unloaded module in \"module:env_name\" style\u001b[39;00m\n\u001b[0;32m--> 741\u001b[0m     env_spec \u001b[39m=\u001b[39m _find_spec(\u001b[39mid\u001b[39;49m)\n\u001b[1;32m    743\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(env_spec, EnvSpec)\n\u001b[1;32m    745\u001b[0m \u001b[39m# Update the env spec kwargs with the `make` kwargs\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/gymnasium/envs/registration.py:527\u001b[0m, in \u001b[0;36m_find_spec\u001b[0;34m(env_id)\u001b[0m\n\u001b[1;32m    521\u001b[0m     logger\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    522\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUsing the latest versioned environment `\u001b[39m\u001b[39m{\u001b[39;00mnew_env_id\u001b[39m}\u001b[39;00m\u001b[39m` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    523\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minstead of the unversioned environment `\u001b[39m\u001b[39m{\u001b[39;00menv_name\u001b[39m}\u001b[39;00m\u001b[39m`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    524\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[39mif\u001b[39;00m env_spec \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     _check_version_exists(ns, name, version)\n\u001b[1;32m    528\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mError(\n\u001b[1;32m    529\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo registered env with id: \u001b[39m\u001b[39m{\u001b[39;00menv_name\u001b[39m}\u001b[39;00m\u001b[39m. Did you register it, or import the package that registers it? Use `gymnasium.pprint_registry()` to see all of the registered environments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    530\u001b[0m     )\n\u001b[1;32m    532\u001b[0m \u001b[39mreturn\u001b[39;00m env_spec\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/gymnasium/envs/registration.py:393\u001b[0m, in \u001b[0;36m_check_version_exists\u001b[0;34m(ns, name, version)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[39mif\u001b[39;00m get_env_id(ns, name, version) \u001b[39min\u001b[39;00m registry:\n\u001b[1;32m    391\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m _check_name_exists(ns, name)\n\u001b[1;32m    394\u001b[0m \u001b[39mif\u001b[39;00m version \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    395\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/gymnasium/envs/registration.py:370\u001b[0m, in \u001b[0;36m_check_name_exists\u001b[0;34m(ns, name)\u001b[0m\n\u001b[1;32m    367\u001b[0m namespace_msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m in namespace \u001b[39m\u001b[39m{\u001b[39;00mns\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m ns \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    368\u001b[0m suggestion_msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Did you mean: `\u001b[39m\u001b[39m{\u001b[39;00msuggestion[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m`?\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m suggestion \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 370\u001b[0m \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mNameNotFound(\n\u001b[1;32m    371\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEnvironment `\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m` doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt exist\u001b[39m\u001b[39m{\u001b[39;00mnamespace_msg\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00msuggestion_msg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m )\n",
      "\u001b[0;31mNameNotFound\u001b[0m: Environment `Breakout` doesn't exist."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "      # Create the environment\n",
    "    env = gym.make(\"Breakout-v4\", render_mode=\"human\")\n",
    "\n",
    "    # Define configuration parameters\n",
    "    config = Config(\n",
    "        observation_space_size=env.observation_space.shape[0],\n",
    "        action_space_size=env.action_space.n,\n",
    "        representation_size=128,\n",
    "        batch_size=64,\n",
    "        learning_rate_prediction=0.001,\n",
    "        update_interval=100,\n",
    "        checkpoint_interval=1000,\n",
    "        replay_buffer_capacity=10000,\n",
    "        input_size=env.observation_space.shape[0],\n",
    "    )\n",
    "\n",
    "    # Create an instance of the Agent class\n",
    "    agent = Agent(config=config )\n",
    "\n",
    "    # Run the agent\n",
    "    agent.run()\n",
    "\n",
    "    # let the agent play the game\n",
    "    agent.play(num_episodes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

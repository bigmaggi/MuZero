{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "2.0.1+cu117\n",
      "0.29.0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "import unittest\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "import time  # Add this import at the beginning of the file\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "# Set up device\n",
    "# device = torch.device(\"mps\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.__version__)\n",
    "print(gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalar_to_onehot(self, action, action_space_size):\n",
    "    action_onehot = torch.zeros(action_space_size).to(device)\n",
    "    action_onehot[action] = 1\n",
    "    return action_onehot.view(1, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepresentationFunction(nn.Module):\n",
    "    def __init__(self, input_size, representation_size):\n",
    "        super(RepresentationFunction, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, representation_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class InitialRepresentationFunction(nn.Module):\n",
    "    def __init__(self, input_size, representation_size):\n",
    "        super(InitialRepresentationFunction, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, representation_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x.to(self.fc1.weight.device)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicsFunction(nn.Module):\n",
    "    def __init__(self, input_size, representation_size, action_space_size):\n",
    "        super(DynamicsFunction, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, representation_size)\n",
    "        self.action_space_size = action_space_size\n",
    "\n",
    "    def forward(self, state_repr, action):\n",
    "        batch_size = state_repr.size(0)\n",
    "        action_onehot = self.scalar_to_onehot(action, self.action_space_size)\n",
    "        x = torch.cat((state_repr, action_onehot), dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def scalar_to_onehot(self, action, action_space_size):\n",
    "        action_onehot = torch.zeros(action_space_size).to(device)\n",
    "        action_onehot[action] = 1\n",
    "        return action_onehot.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionFunction(nn.Module):\n",
    "    def __init__(self, representation_size, action_space_size):\n",
    "        super(PredictionFunction, self).__init__()\n",
    "        self.fc1 = nn.Linear(representation_size, 128)\n",
    "        self.fc2 = nn.Linear(128, action_space_size)\n",
    "\n",
    "    def forward(self, state_repr):\n",
    "        x = F.relu(self.fc1(state_repr))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.environment_name = \"LunarLander-v2\"\n",
    "        self.render_mode = \"human\"  # \"human\" or \"rgb_array\"\n",
    "        self.seed = 42069\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.training_steps = 10000\n",
    "        self.lr = 0.001\n",
    "        self.representation_size = 64\n",
    "        self.action_space_size = 4\n",
    "        self.min_priority = 0.1\n",
    "        self.checkpoint_interval = 1000\n",
    "        self.num_unroll_steps = 5\n",
    "        self.td_steps = 10\n",
    "        self.discount = 0.997\n",
    "        self.exploration_constant = 1.0\n",
    "        self.learning_rate = 0.001\n",
    "        self.representation_size = 64\n",
    "        self.batch_size = 128\n",
    "        self.num_epochs = 10\n",
    "        self.replay_buffer_capacity = 1000\n",
    "        self.alpha = 0.6\n",
    "        self.beta = 0.4\n",
    "        self.beta_increment = 0.001\n",
    "        self.eps = 0.01\n",
    "        self.num_simulations = 50\n",
    "        self.temperature = 1.0\n",
    "        self.dirichlet_alpha = 0.25\n",
    "        self.noise_weight = 0.25\n",
    "        self.gradient_clip = 40.0\n",
    "        self.max_moves = 27000\n",
    "        self.mcts_discount = self.discount\n",
    "        self.episodes = 100000\n",
    "        self.gamma = 0  # Discount factor for the Bellman equation\n",
    "        self.lr_repr = 0.0001\n",
    "        self.lr_dyn = 0.0001\n",
    "        self.lr_pred = 0.0001\n",
    "        self.update_interval = 10\n",
    "        self.max_steps = 200\n",
    "        self.max_episode_length = 200\n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_end = 0.1\n",
    "        self.epsilon_decay = 200\n",
    "        self.start_steps = 100\n",
    "        self.reset_intervall = 750\n",
    "        self.action_space_size = 4\n",
    "        self.replay_initial = 1000\n",
    "\n",
    "config = Config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, hidden_state, reward, terminal, action_space):\n",
    "        self.hidden_state = hidden_state\n",
    "        self.reward = reward\n",
    "        self.terminal = terminal\n",
    "        self.children = [None] * action_space\n",
    "        self.total_value = [0] * action_space\n",
    "        self.visit_count = [0] * action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    def __init__(self, action_space_size, representation_function, dynamics_function, prediction_function):\n",
    "        self.action_space_size = action_space_size\n",
    "        self.num_simulations = config.num_simulations\n",
    "        self.discount = config.mcts_discount\n",
    "        self.root = None\n",
    "        self.dynamics_function = dynamics_function\n",
    "        self.prediction_function = prediction_function\n",
    "        self.exploration_constant = config.exploration_constant\n",
    "\n",
    "    def UCB_score(self, node, action):\n",
    "        if node.visit_count[action] == 0:\n",
    "            return float('inf')\n",
    "        else:\n",
    "            # Use the model to predict the value of the action\n",
    "            state = node.hidden_state\n",
    "            predicted_values = self.prediction_function(state)\n",
    "            Q = predicted_values[0][action]\n",
    "            U = self.exploration_constant * math.sqrt(math.log(sum(node.visit_count)) / node.visit_count[action])\n",
    "            return Q + U\n",
    "\n",
    "    def expand(self, node, action):\n",
    "        next_state, reward = self.dynamics_function(node.hidden_state, torch.tensor([action], dtype=torch.float32).to(device))\n",
    "        next_state = next_state.clone().detach().to(device)\n",
    "        reward = reward.item()\n",
    "        return Node(next_state, reward, False, self.action_space_size)\n",
    "\n",
    "    def backpropagate(self, leaf_value, path):\n",
    "        for node, action in reversed(path):\n",
    "            node.visit_count[action] += 1\n",
    "            node.total_value[action] += leaf_value\n",
    "            leaf_value *= self.discount\n",
    "\n",
    "    def run(self, initial_state):\n",
    "        # Create root node with initial state\n",
    "        initial_hidden_state = initial_state\n",
    "        self.root = Node(initial_hidden_state, 0, False, self.action_space_size)\n",
    "\n",
    "        for _ in range(self.num_simulations):\n",
    "            node, path = self.root, []\n",
    "            while node is not None:\n",
    "                best_action = max(range(self.action_space_size), key=lambda a: self.UCB_score(node, a))\n",
    "                path.append((node, best_action))\n",
    "                if node.children[best_action] is None:\n",
    "                    node.children[best_action] = self.expand(node, best_action)\n",
    "                node = node.children[best_action]\n",
    "\n",
    "            leaf = path[-1][0]\n",
    "            self.backpropagate(leaf.reward, path)\n",
    "\n",
    "        return self.root\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, experience):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = experience\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer(ReplayBuffer):\n",
    "    def __init__(self, capacity, alpha=0.6, beta=0.4, beta_increment=0.001, eps=0.01):\n",
    "        super().__init__(capacity)\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.beta_increment = beta_increment\n",
    "        self.eps = eps\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "\n",
    "    def push(self, experience):\n",
    "        max_priority = self.priorities.max() if self.buffer else 1.0\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(experience)\n",
    "        else:\n",
    "            self.buffer[self.position] = experience\n",
    "        self.priorities[self.position] = max_priority\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            priorities = self.priorities\n",
    "        else:\n",
    "            priorities = self.priorities[:self.position]\n",
    "        probabilities = priorities ** self.alpha\n",
    "        probabilities /= probabilities.sum()\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)\n",
    "        experiences = [self.buffer[idx] for idx in indices]\n",
    "        weights = (len(self.buffer) * probabilities[indices]) ** (-self.beta)\n",
    "        weights /= weights.max()\n",
    "        self.beta = min(1.0, self.beta + self.beta_increment)\n",
    "        return experiences, indices, np.array(weights, dtype=np.float32)\n",
    "\n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, priority in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = priority\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, config, representation_size, action_space_size):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.env = gym.make(config.environment_name, render_mode=config.render_mode)\n",
    "        self.representation_function = RepresentationFunction(representation_size=representation_size, input_size=self.env.observation_space.shape[0]).to(self.device)\n",
    "        self.representation_size = representation_size\n",
    "        self.dynamics_input_size = representation_size + action_space_size\n",
    "        self.action_size = action_space_size\n",
    "        self.dynamics_function = DynamicsFunction(self.dynamics_input_size, representation_size, self.action_size).to(device)\n",
    "        self.prediction_function = PredictionFunction(self.representation_size, action_space_size).to(self.device)\n",
    "        self.action_space_size = action_space_size\n",
    "\n",
    "        self.optimizer_representation = optim.Adam(self.representation_function.parameters(), lr=config.learning_rate)\n",
    "        self.optimizer_dynamics = optim.Adam(self.dynamics_function.parameters(), lr=config.learning_rate)\n",
    "        self.optimizer_prediction = optim.Adam(self.prediction_function.parameters(), lr=config.learning_rate)\n",
    "        self.total_steps = 0\n",
    "        self.training_steps_completed = 0\n",
    "        self.mcts = MCTS(action_space_size=config.action_space_size, representation_function=self.representation_function, dynamics_function=self.dynamics_function, prediction_function=self.prediction_function)\n",
    "        self.replay_buffer = ReplayBuffer(config.replay_buffer_capacity)\n",
    "\n",
    "    def get_action(self, state_repr, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(self.env.action_space.n)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # Create a tensor to store the Q values for all possible actions\n",
    "                q_values = torch.empty(self.env.action_space.n).to(self.device)\n",
    "                # Calculate Q value for each action\n",
    "                for action in range(self.env.action_space.n):\n",
    "                    action_one_hot = F.one_hot(torch.tensor([action]), self.env.action_space.n).float().to(self.device)\n",
    "                    # Concatenate state and action and feed to prediction function\n",
    "                    state_action = torch.cat([state_repr, action_one_hot], dim=1)\n",
    "                    q_values[action] = self.prediction_function(state_action)\n",
    "                # Choose the action with the highest Q value\n",
    "                action = torch.argmax(q_values).item()  # Returns a Python integer\n",
    "                return action\n",
    "\n",
    "    def print_progress(self, current_step, total_steps):\n",
    "        print(f\"Training Agent: {current_step}/{total_steps} steps completed\")\n",
    "\n",
    "    def animate(self):\n",
    "        state = self.env.reset()\n",
    "        print(state)\n",
    "        state_repr = self.representation_function(torch.tensor(state[0], dtype=torch.float32).unsqueeze(0).to(self.device))\n",
    "        frames = []\n",
    "        done = False  # Initialize done before using it in the loop\n",
    "        while not done:\n",
    "            action = self.get_action(state_repr, max(self.config.epsilon_end, self.config.epsilon_start - self.total_steps / self.config.epsilon_decay))\n",
    "            next_state, _, done, _, _ = self.env.step(action)  # Extract the integer action value using .item()\n",
    "            next_state_repr = self.representation_function(torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(self.device))\n",
    "            # Render the environment as RGB array\n",
    "            frame = self.env.render()\n",
    "            frames.append([plt.imshow(frame, animated=True)])\n",
    "            state_repr = next_state_repr\n",
    "            self.total_steps += 1\n",
    "        plt.show()\n",
    "        return animation.ArtistAnimation(plt.gcf(), frames, interval=50, blit=True, repeat_delay=1000)\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.replay_buffer) < self.config.replay_initial:\n",
    "            return\n",
    "    \n",
    "        state, _ = self.env.reset()\n",
    "        state_repr = self.representation_function(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device))\n",
    "        done = False\n",
    "    \n",
    "        while not done:\n",
    "            action = self.get_action(state_repr, self.config.epsilon_end)\n",
    "            next_state, reward, done, _, _ = self.env.step(action)\n",
    "            next_state_repr = self.representation_function(torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(self.device))\n",
    "    \n",
    "            self.replay_buffer.push((state_repr.detach().cpu().numpy(), action, reward, next_state_repr.detach().cpu().numpy(), done))\n",
    "            state_repr = next_state_repr\n",
    "    \n",
    "            if done:\n",
    "                state, _ = self.env.reset()\n",
    "                state_repr = self.representation_function(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device))\n",
    "    \n",
    "            if len(self.replay_buffer) >= self.config.replay_initial:\n",
    "                self.update_parameters()\n",
    "\n",
    "\n",
    "    \n",
    "    def run(self):\n",
    "        while self.training_steps_completed < self.config.training_steps:\n",
    "            state = self.env.reset()\n",
    "            state_repr = self.representation_function(torch.tensor(state, dtype=torch.float32).to(self.device))  # Remove unsqueeze(0)\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.get_action(state_repr, max(self.config.epsilon_end, self.config.epsilon_start - self.total_steps / self.config.epsilon_decay))\n",
    "                next_state, reward, done, _, _ = self.env.step(action)  # Convert action tensor to integer\n",
    "                next_state_repr = self.representation_function(torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(self.device))\n",
    "                self.replay_buffer.push((state_repr.detach().cpu().numpy(), action, reward, next_state_repr.detach().cpu().numpy(), done))\n",
    "                state_repr = next_state_repr\n",
    "                self.total_steps += 1\n",
    "                if self.total_steps % self.config.update_interval == 0:\n",
    "                    self.train()\n",
    "            self.training_steps_completed += 1\n",
    "            if self.training_steps_completed % self.config.checkpoint_interval == 0:\n",
    "                self.save_checkpoint()\n",
    "\n",
    "    def populate_initial_buffer(self):\n",
    "        state, _ = self.env.reset()\n",
    "        print(type(state), state)\n",
    "        print(\"Initial state:\", state)  \n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        print(\"State tensor shape:\", state_tensor.shape)  # Add this line\n",
    "        state_repr = self.representation_function(state_tensor)\n",
    "        done = False\n",
    "    \n",
    "        while len(self.replay_buffer) < self.config.replay_initial:\n",
    "            action = self.get_action(state_repr, max(self.config.epsilon_end, self.config.epsilon_start - self.total_steps / self.config.epsilon_decay))\n",
    "            next_state, reward, done, _, _ = self.env.step(action)  # Convert action tensor to integer\n",
    "            next_state_repr = self.representation_function(torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(self.device))\n",
    "            self.replay_buffer.push((state_repr.detach().cpu().numpy(), action, reward, next_state_repr.detach().cpu().numpy(), done))\n",
    "    \n",
    "            if done:\n",
    "                state, _ = self.env.reset()\n",
    "                state_repr = self.representation_function(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device))\n",
    "            else:\n",
    "                state_repr = next_state_repr\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(self.representation_function.state_dict(), 'representation_function.pth')\n",
    "        torch.save(self.dynamics_function.state_dict(), 'dynamics_function.pth')\n",
    "        torch.save(self.prediction_function.state_dict(), 'prediction_function.pth')\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        if os.path.isfile('representation_function.pth'):\n",
    "            self.representation_function.load_state_dict(torch.load('representation_function.pth'))\n",
    "        if os.path.isfile('dynamics_function.pth'):\n",
    "            self.dynamics_function.load_state_dict(torch.load('dynamics_function.pth'))\n",
    "        if os.path.isfile('prediction_function.pth'):\n",
    "            self.prediction_function.load_state_dict(torch.load('prediction_function.pth'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EEEE\n",
      "======================================================================\n",
      "ERROR: test_dynamics_function (__main__.TestFunctions.test_dynamics_function)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3270/3011834528.py\", line 7, in setUp\n",
      "    self.agent = Agent(self.config, self.representation_size)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: Agent.__init__() missing 1 required positional argument: 'action_space_size'\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_initial_representation_function (__main__.TestFunctions.test_initial_representation_function)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3270/3011834528.py\", line 7, in setUp\n",
      "    self.agent = Agent(self.config, self.representation_size)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: Agent.__init__() missing 1 required positional argument: 'action_space_size'\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_prediction_function (__main__.TestFunctions.test_prediction_function)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3270/3011834528.py\", line 7, in setUp\n",
      "    self.agent = Agent(self.config, self.representation_size)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: Agent.__init__() missing 1 required positional argument: 'action_space_size'\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_representation_function (__main__.TestFunctions.test_representation_function)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3270/3011834528.py\", line 7, in setUp\n",
      "    self.agent = Agent(self.config, self.representation_size)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: Agent.__init__() missing 1 required positional argument: 'action_space_size'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.003s\n",
      "\n",
      "FAILED (errors=4)\n"
     ]
    }
   ],
   "source": [
    "class TestFunctions(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.config = Config()\n",
    "        self.env = gym.make(self.config.environment_name, render_mode=self.config.render_mode)\n",
    "        self.representation_size = 128\n",
    "        action_space_size = self.env.action_space.n\n",
    "        self.agent = Agent(self.config, self.representation_size)\n",
    "        self.representation_function = RepresentationFunction(self.env.observation_space.shape[0], self.representation_size).to(device)\n",
    "        self.dynamics_function = DynamicsFunction(self.representation_size + action_space_size, self.representation_size).to(device)\n",
    "        self.prediction_function = PredictionFunction(self.representation_size, action_space_size).to(device)\n",
    "\n",
    "    def test_representation_function(self):\n",
    "        representation_function = RepresentationFunction(self.env.observation_space.shape[0], self.representation_size).to(device)\n",
    "        input_tensor = torch.randn(1, self.env.observation_space.shape[0]).to(device)\n",
    "        output_tensor = representation_function(input_tensor)\n",
    "        self.assertEqual(output_tensor.shape, (1, self.representation_size))\n",
    "\n",
    "    def test_initial_representation_function(self):\n",
    "        state, _ = self.env.reset()\n",
    "        state_tensor = torch.tensor(state).float().unsqueeze(0).to(device)\n",
    "        output_tensor = self.agent.representation_function(state_tensor)\n",
    "        self.assertEqual(output_tensor.shape, (1, self.representation_size))\n",
    "\n",
    "    def test_dynamics_function(self):\n",
    "        state_repr = torch.randn(1, self.representation_size).to(device)\n",
    "        action = torch.randn(1, self.env.action_space.n).to(device)\n",
    "        output_tensor = self.agent.dynamics_function(state_repr, action)\n",
    "        self.assertEqual(output_tensor.shape, (1, self.representation_size))\n",
    "\n",
    "    def test_prediction_function(self):\n",
    "        input_tensor = torch.randn(1, self.representation_size).to(device)\n",
    "        output_tensor = self.agent.prediction_function(input_tensor)\n",
    "        self.assertEqual(output_tensor.shape, (1, self.env.action_space.n))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and run the test case\n",
    "    suite = unittest.TestLoader().loadTestsFromTestCase(TestFunctions)\n",
    "    result = unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run it all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment observation space: (8,)\n",
      "Agent representation function: RepresentationFunction(\n",
      "  (fc1): Linear(in_features=8, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      ")\n",
      "<class 'numpy.ndarray'> [ 0.00586824  1.4191151   0.59436554  0.36420003 -0.00679295 -0.13463262\n",
      "  0.          0.        ]\n",
      "Initial state: [ 0.00586824  1.4191151   0.59436554  0.36420003 -0.00679295 -0.13463262\n",
      "  0.          0.        ]\n",
      "State tensor shape: torch.Size([1, 8])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x128 and 132x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[479], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAgent representation function:\u001b[39m\u001b[39m\"\u001b[39m, agent\u001b[39m.\u001b[39mrepresentation_function)\n\u001b[1;32m     31\u001b[0m agent\u001b[39m.\u001b[39mpopulate_initial_buffer()\n\u001b[0;32m---> 32\u001b[0m agent\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     34\u001b[0m \u001b[39m# Visualization of agent's interactions with the environment\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[477], line 63\u001b[0m, in \u001b[0;36mAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m---> 63\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_action(state_repr, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mepsilon_end)\n\u001b[1;32m     64\u001b[0m     next_state, reward, done, _, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m     65\u001b[0m     next_state_repr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepresentation_function(torch\u001b[39m.\u001b[39mtensor(next_state, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice))\n",
      "Cell \u001b[0;32mIn[477], line 27\u001b[0m, in \u001b[0;36mAgent.get_action\u001b[0;34m(self, state, epsilon)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 27\u001b[0m         Q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprediction_function(state)\n\u001b[1;32m     28\u001b[0m         action \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(Q_values)\u001b[39m.\u001b[39mitem()  \u001b[39m# Returns a Python integer\u001b[39;00m\n\u001b[1;32m     29\u001b[0m         \u001b[39mreturn\u001b[39;00m action\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[472], line 8\u001b[0m, in \u001b[0;36mPredictionFunction.forward\u001b[0;34m(self, state_repr)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, state_repr):\n\u001b[0;32m----> 8\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(state_repr))\n\u001b[1;32m      9\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x)\n\u001b[1;32m     10\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x128 and 132x128)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config = Config()\n",
    "    env = gym.make(config.environment_name, render_mode=config.render_mode)\n",
    "    print(\"Environment observation space:\", env.observation_space.shape)\n",
    "    input_size = env.observation_space.shape[0]\n",
    "    representation_size = 128\n",
    "    action_size = env.action_space.n\n",
    "    representation_function = RepresentationFunction(input_size, representation_size).to(device)\n",
    "    dynamics_input_size = representation_size + action_size\n",
    "    dynamics_function = DynamicsFunction(dynamics_input_size, representation_size, action_size).to(device)\n",
    "    prediction_input_size = representation_size + action_size\n",
    "    prediction_output_size = 65\n",
    "    prediction_function = PredictionFunction(prediction_input_size, prediction_output_size).to(device)\n",
    "    \n",
    "    agent = Agent(config, representation_size, action_size)  # Add 'action_size' argument here\n",
    "\n",
    "    if os.path.isfile('representation_function.pth'):\n",
    "        representation_function.load_state_dict(torch.load('representation_function.pth'))\n",
    "    if os.path.isfile('dynamics_function.pth'):\n",
    "        dynamics_function.load_state_dict(torch.load('dynamics_function.pth'))\n",
    "    if os.path.isfile('prediction_function.pth'):\n",
    "        prediction_function.load_state_dict(torch.load('prediction_function.pth'))\n",
    "\n",
    "    agent.representation_function = representation_function\n",
    "    agent.dynamics_function = dynamics_function\n",
    "    agent.prediction_function = prediction_function\n",
    "\n",
    "    print(\"Agent representation function:\", agent.representation_function)\n",
    "\n",
    "    \n",
    "    agent.populate_initial_buffer()\n",
    "    agent.train()\n",
    "\n",
    "    # Visualization of agent's interactions with the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
